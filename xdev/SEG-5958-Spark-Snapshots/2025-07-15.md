# 2025-06-30

## Create the parquet

Cluster we use: <https://us-west-2.console.aws.amazon.com/emr/home?region=us-west-2#/clusterDetails/j-1I822RFZ29ZHY>

Enter de cluster `aws ssm start-session --region us-west-2 --target i-001e7f63c2e21d07c`

```shell
sudo su hadoop
cd
```

This command is for run the job

spark-submit --master yarn --deploy-mode client --conf spark.yarn.maxAppAttempts=1 --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.executor.instances=0 --driver-memory 10g --jars s3://adroll-releases/data/core-spark/core-spark.jar --py-files s3://adroll-test-sandbox-2/lucas.visser/kappa/kappa.zip s3://adroll-test-sandbox-2/lucas.visser/kappa/jobs/crossdevice/crossdevice_snapshot.py --namespace ip_to_domain --logs-date 2025-07-12 --output-prefix s3://adroll-crossdevice/tmp/test/snapshots-parquet/namespace=ip_to_domain/date=2025-07-12/

## Test

Cluster -> <https://us-west-2.console.aws.amazon.com/emr/home?region=us-west-2#/clusterDetails/j-1LELHX9D1HV5O>

`pyspark --master yarn --deploy-mode client --conf spark.yarn.maxAppAttempts=1 --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.executor.instances=0 --driver-memory 10g --jars s3://adroll-releases/data/core-spark/core-spark.jar --py-files s3://adroll-test-sandbox-2/lucas.visser/kappa/kappa.zip
`

`from kappa.jobs.crossdevice.crossdevice_snapshot import *`

`df = spark.read.parquet("s3://adroll-crossdevice/tmp/test/snapshots-parquet/namespace=ip_to_domain/date=2025-07-12/")`

`df.show(10)`

+--------------------+-------+--------------------+----------+------------------+--------------------+-----+
|                  id|id_type|          cluster_id|        ts|transaction_log_ts|            metadata|shard|
+--------------------+-------+--------------------+----------+------------------+--------------------+-----+
|2401:4900:882f:fe...|     Ip|1f28558edc87cd20f...|1726531200|        1726765849|{"sources": ["idn...| 0020|
|2401:4900:882f:ff...|     Ip|1ecedcf56c4d7ff3e...|1723075200|        1723132783|{"sources": ["idn...| 0020|
|2401:4900:882f:ff...|     Ip|e0c8498853106fbf9...|1722729600|        1723079240|{"sources": ["idn...| 0020|
|2401:4900:8830:16...|     Ip|11bf8b0ee4016c06a...|1745971200|        1746025989|{"sources":["idn"...| 0020|
|2401:4900:8830:26...|     Ip|25346d695e6274e9a...|1746662400|        1746709797|{"sources":["idn"...| 0020|
|2401:4900:8830:2c...|     Ip|e2bbb162a6eb9e1ae...|1745712000|        1745785574|{"sources":["idn"...| 0020|
|2401:4900:8830:2d...|     Ip|415a90edd175c917e...|1743811200|        1743874892|{"sources":["idn"...| 0020|
|2401:4900:8830:33...|     Ip|43269874281832691...|1747526400|        1747598421|{"sources":["idn"...| 0020|
|2401:4900:8830:36...|     Ip|ca00d7fbd5b1eeb80...|1741219200|        1741257862|{"sources":["idn"...| 0020|
|2401:4900:8830:37...|     Ip|a1cd3f219c7f23b96...|1750464000|        1750475056|{"sources":["idn"...| 0020|
+--------------------+-------+--------------------+----------+------------------+--------------------+-----+
only showing top 10 rows

## Parquet snapshot 2025-07-13

New parquet with a new code

spark-submit --master yarn --deploy-mode client --conf spark.yarn.maxAppAttempts=1 --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.executor.instances=0 --driver-memory 10g --jars s3://adroll-releases/data/core-spark/core-spark.jar --py-files s3://adroll-test-sandbox-2/lucas.visser/kappa/kappa.zip s3://adroll-test-sandbox-2/lucas.visser/kappa/jobs/crossdevice/crossdevice_snapshot.py --namespace ip_to_domain --logs-date 2025-07-14 --output-prefix s3://adroll-crossdevice/tmp/test/snapshots-parquet/namespace=ip_to_domain/date=2025-07-14/
